{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data Acquisition\n",
    "\n",
    "This code downloads the [bchydro-outages](https://github.com/outages/bchydro-outages/tree/main) project (including all commit history) from GitHub and saves it to a sub directory.\n",
    "\n",
    "Then it does some work to process it into a usable Pandas-compatible format.\n",
    "\n",
    "See BC Hydro's frontend here: https://www.bchydro.com/power-outages/app/outage-map.html\n",
    "\n",
    "\n",
    "> If, when you run this notebook you get an error about undefined variables. Just \"Run All\" again and it should work the second time.\n",
    ">\n",
    "> Also try deleting the cached \"bchydro-outages\" directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config Notes\n",
    "- 14 days took ~30 seconds to run on my computer, plus ~15 seconds to download the repo - Bea\n",
    "- 90 days took ~10 minutes to run on my less good laptop - Bea\n",
    "- 30 days took ~10 sec to run on my mac m1 air, total size 1.7 MB - Soumya\n",
    "- 150 days took ~2 minutes and 52 sec to run, total size 18.9 MB - Soumya\n",
    "- 200 days took ~4 minutes and 18 sec to run, total size 22.7 MB - Soumya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be wary of how much compute big numbers require\n",
    "# The file size of the CSV will probably never be more than 200Mb\n",
    "# DAYS_TO_CAPTURE=('January 1st, 2024', \"January 1st, 2025\")\n",
    "\n",
    "# The date range of outages to scrape from the outages repository\n",
    "DAYS_TO_CAPTURE=('January 1st, 2024', \"January 2nd, 2025\")\n",
    "\n",
    "# If True, deletes the old repository data and starts fresh\n",
    "DELETE_OLD_REPO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "target = os.getcwd()\n",
    "repoName = \"bchydro-outages\"\n",
    "\n",
    "repoPath = os.path.join(target, repoName)\n",
    "\n",
    "if DELETE_OLD_REPO and os.path.exists(repoPath):\n",
    "  # https://stackoverflow.com/a/6996628 <-- How to delete a directory in Python\n",
    "  shutil.rmtree(repoPath)\n",
    "\n",
    "# https://stackoverflow.com/a/4760517 <-- How to run subprocess in Python\n",
    "\n",
    "# Clone the REPO and save to <repoName> folder under DataAcquisition/\n",
    "result = subprocess.run(\n",
    "  [\"git\", \"clone\", \"https://github.com/outages/bchydro-outages.git\", f\"./{repoName}\"],\n",
    "  cwd=target,\n",
    "  capture_output=True,\n",
    ")\n",
    "print(result.stdout.decode(\"utf-8\"))\n",
    "print(result.stderr.decode(\"utf-8\"))\n",
    "\n",
    "# Confirm that the repository was cloned\n",
    "assert os.path.exists(repoPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process commit history\n",
    "\n",
    "The repo only has 1 file in it: a `.json` file which shows the current (right now) outages being tracked by BC Hydro. To find historical data, we need to traverse the commit history and merge each commit together\n",
    "\n",
    "\n",
    "### Step 1) Get the JSON data from each commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Fun progress bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# Reset to main branch\n",
    "subprocess.run([\"git\", \"checkout\", \"main\"], cwd=repoPath)\n",
    "\n",
    "commitLog = subprocess.run(\n",
    "  [\"git\", \"log\", f\"--before=\\\"{DAYS_TO_CAPTURE[1]}\\\"\", f\"--since=\\\"{DAYS_TO_CAPTURE[0]}\\\"\", \"--pretty=format:%H:%ct\", \"--reverse\"],\n",
    "  cwd=repoPath,\n",
    "  capture_output=True,\n",
    ")\n",
    "commits = commitLog.stdout.decode(\"utf-8\")\n",
    "\n",
    "commits = [commit.split(\":\") for commit in commits.split(\"\\n\")]\n",
    "\n",
    "print(f\"Number of commits scraped: {len(commits)}\")\n",
    "\n",
    "# Get JSON file for each commit\n",
    "\n",
    "OUTAGES_FILE_NAME = \"bchydro-outages.json\"\n",
    "outagesFilePath = os.path.join(repoPath, OUTAGES_FILE_NAME)\n",
    "\n",
    "\n",
    "def getJSON(commit):\n",
    "  subprocess.run([\"git\", \"checkout\", commit[0]], cwd=repoPath, capture_output=True)\n",
    "  assert os.path.exists(os.path.join(repoPath, outagesFilePath))\n",
    "  try:\n",
    "    with open(outagesFilePath, \"r\") as f:\n",
    "      # https://stackoverflow.com/q/20199126 <-- How to load JSON from a file\n",
    "      return json.load(f)\n",
    "  except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON for commit hash: {commit[0]}\")\n",
    "    return None\n",
    "    \n",
    "\n",
    "maxProg = len(commits)\n",
    "prog = IntProgress(min=0, max=maxProg)\n",
    "display(prog)\n",
    "\n",
    "jsonData = []\n",
    "for commit in commits:\n",
    "  prog.value += 1\n",
    "  jsonData.append(getJSON(commit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Combine the JSON data\n",
    "\n",
    "Because this tracks current outages, we never actually get to see the \"timeOn\" at the end. The `dateOn` field in the JSON is only an estimate time. \n",
    "\n",
    "But, we can interpret when the outage ended (within +-15 minutes) by seeing if it is present in the next commit. This is why we'll start from the latest commit and work towards present-day\n",
    "\n",
    "This does mean that the most recent outages (any that are still ongoing) won't be added to the list, but that's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to take all but start time from the latest entry for an outage\n",
    "outages = []\n",
    "\n",
    "\n",
    "waitingForEndTime = []\n",
    "for i, data in enumerate(jsonData):\n",
    "  if data is None:\n",
    "    continue\n",
    "\n",
    "\n",
    "  commitTime = commits[i][1]\n",
    "\n",
    "  # data is a list of active outages\n",
    "  activeOutageIds = [outage[\"id\"] for outage in data]\n",
    "\n",
    "  # Check if any of the outages in waitingForEndTime are in the active outages\n",
    "  # Push to final array if they aren't\n",
    "\n",
    "  newWaiting = []\n",
    "  for waitingOutage in waitingForEndTime:\n",
    "    if waitingOutage[\"id\"] not in activeOutageIds:\n",
    "      waitingOutage[\"endTime\"] = commitTime\n",
    "      outages.append(waitingOutage)\n",
    "    else:\n",
    "      newWaiting.append(waitingOutage)\n",
    "  waitingForEndTime = newWaiting\n",
    "\n",
    "  # Override any outages waiting for time with latest data\n",
    "  for i, waitingOutage in enumerate(waitingForEndTime):\n",
    "    for outage in data:\n",
    "      if waitingOutage[\"id\"] == outage[\"id\"]:\n",
    "        waitingForEndTime[i] = outage\n",
    "        break\n",
    "\n",
    "  # Add any new outages to waitingForEndTime\n",
    "  for outage in data:\n",
    "    if outage[\"id\"] not in [waitingOutage[\"id\"] for waitingOutage in waitingForEndTime]:\n",
    "      waitingForEndTime.append(outage)\n",
    "\n",
    "print(f\"Raw Outages Indexed: {len(outages)}\")\n",
    "print(f\"Active Outages (Not recorded): {len(waitingForEndTime)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Cleanup \n",
    "\n",
    "Delete unneeded fields and convert all times to proper datetime objects\n",
    "\n",
    "Some terminology:\n",
    "- \"eta\" is the estimated time of arrival for the repair crew\n",
    "- \"etr\" is the estimated time of restoration for the power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from common import timeStampFields\n",
    "\n",
    "pdOutages = pd.DataFrame(outages)\n",
    "\n",
    "# dateOn is BC Hydro's estimate of when power would be restored, `endTime` is the actual measure time from when it was removed from the active outages page\n",
    "# So let's rename `dateOn` to `estDateOn` and `endTime` to `dateOn`\n",
    "pdOutages.rename(\n",
    "  {\"dateOn\": \"estDateOn\", \"endTime\": \"dateOn\"}, axis=\"columns\", inplace=True\n",
    ")\n",
    "\n",
    "# Let's delete the columns that aren't useful\n",
    "pdOutages.drop(\n",
    "  columns=[\n",
    "    \"showEta\",  # Only useful for BC Hydro's website\n",
    "    \"showEtr\",  # Only useful for BC Hydro's website\n",
    "    \"crewStatusNote\",  # Not specified format (and usually empty)\n",
    "    \"crewStatusDescription\",  # Doesn't update once outage is resolved\n",
    "    \"crewStatus\",  # Doesn't update once outage is resolved\n",
    "    \"polygon\"\n",
    "  ],\n",
    "  inplace=True\n",
    ")\n",
    "\n",
    "# Convert to datetime (some timestamps are in ms unix and some are in s unix so this accounts for that)\n",
    "for field, unit in timeStampFields:\n",
    "  pdOutages[field] = pd.to_datetime(pdOutages[field], unit=unit, utc=True)\n",
    "\n",
    "# Sort\n",
    "pdOutages = pdOutages.sort_values(by=['dateOff', 'dateOn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) Deduplicate Outages \n",
    "\n",
    "Sometimes, outages are duplicated in BC Hydro's own data (ie, the same outage is reported twice __with two different IDs__)\n",
    "\n",
    "We should be able to fix this, though, by grouping all outages that have overlapping start and end times with an identical location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removedDups = 0\n",
    "\n",
    "\n",
    "def matchIntervals(areaEntries: pd.DataFrame):\n",
    "  # Group into intervals\n",
    "  # https://stackoverflow.com/a/48243958\n",
    "  intervals = (\n",
    "    (\n",
    "      areaEntries[\"dateOn\"]\n",
    "      .apply(lambda x: x.timestamp())\n",
    "      .rolling(window=2, min_periods=1)\n",
    "      .min()\n",
    "      - areaEntries[\"dateOff\"]\n",
    "      .apply(lambda x: x.timestamp())\n",
    "      .rolling(window=2, min_periods=1)\n",
    "      .max()\n",
    "    )\n",
    "    < 0\n",
    "  ).cumsum()\n",
    "\n",
    "  areaEntries[\"interval\"] = intervals\n",
    "\n",
    "  global removedDups\n",
    "  removedDups += intervals.count() - len(intervals.unique())\n",
    "\n",
    "  return areaEntries\n",
    "\n",
    "\n",
    "mergedPdOutages = (\n",
    "  pdOutages.groupby(by=[\"area\"])\n",
    "  .apply(matchIntervals)\n",
    "  .reset_index(drop=True)\n",
    "  .groupby(by=[\"area\", \"interval\"])\n",
    "  .aggregate(\n",
    "    {\n",
    "      \"id\": \"first\",  # Maybe not the best idea, perhaps combine ids?\n",
    "      \"gisId\": \"first\",\n",
    "      \"regionId\": \"first\",\n",
    "      \"municipality\": \"first\",\n",
    "      \"area\": \"first\",\n",
    "      \"cause\": \"last\",\n",
    "      \"numCustomersOut\": \"max\",\n",
    "      \"crewEta\": \"max\",\n",
    "      \"crewEtr\": \"max\",\n",
    "      \"dateOff\": \"min\",\n",
    "      \"dateOn\": \"max\",\n",
    "      \"estDateOn\": \"max\",\n",
    "      \"lastUpdated\": \"max\",\n",
    "      \"regionName\": \"first\",\n",
    "      \"latitude\": \"first\",\n",
    "      \"longitude\": \"first\",\n",
    "      \"interval\": \"max\",\n",
    "    }\n",
    "  )\n",
    "  .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Merged {removedDups} outages with overlapping start/end times and areas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import csvFileName\n",
    "\n",
    "pdOutages.to_csv(csvFileName, index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
